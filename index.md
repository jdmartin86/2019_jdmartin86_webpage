# About
I'm a PhD student researching decision making and artificial intelligence at [Stevens Institute of Technology](http://www.stevens.edu). I work as a member of the [Robust Field Autonomy Lab](http://personal.stevens.edu/~benglot/), which is advised by [Prof. Brendan Englot](https://web.stevens.edu/facultyprofile/?id=2043). During the three years preceeding my PhD, I took graduate courses at [Columbia University](https://www.columbia.edu) while also working at [Sikorsky Aircraft](https://www.wired.com/story/sikorsky-sara-helicopter-autonomous-flying-car-air-taxi-tech/), automating a full-scale helicopter. I studied physics as an undergraduate at the [University of Maryland](https://umdphysics.umd.edu). I also earned a second bachelor's degree in [Aerospace Engineering](https://aero.umd.edu). See my [CV](/2019-martin-cv.pdf) for more details.

# Research Interests
I want to understand how we can design machines that are capable of learning and making intelligent decisions. I like to think about new decision processes that would help autonomous agents reason about their environment and task in better ways. Some other questions I find interesting are: How can machines learn as efficiently as humans through streams of reinforcement? What are the inductive pathways that make efficient learning possible? And how can those mechanisms be formalized and encoded into programs that solve meaningful problems?  

I believe the Reinforcement Learning paradigm provides a good framework from which many of these questions can be studied. In my career, I hope to address some of RL's fundamental limitations by expanding its scope to new settings and larger, more complex tasks. Specifically, I would like to understand what logic underlies reliable generalization between different but related tasks, in tasks that evolve slowly over time, in tasks that continue indefinitely, and in tasks which can only be attempted a few times, maybe even once. How can multiple agents learn in harmony or in competition? I'm motivated and inspired by the work of my peers in optimization, statistical machine learning, cognitive science, and mathematical psychology. Though I wouldn't claim RL is the only way to study decision making, I do believe that advances in RL will lead to more powerful AI systems and better theoretical paradigms from which others can understand human cognition. 

# News
* **October 22, 2019:** I'm giving a talk at [DeepMind](http://deepmind.com), Edmonton about how RL agents can benefit from organizing their experience into contexts.
* **October 7, 2019:** Two of my projects will appear as NeurIPS workshop papers this year.   * [Saftey and Robust Decison Making Workshop](https://sites.google.com/view/neurips19-safe-robust-workshop#h.p_iF36C6BL_elR): Stochastically Dominant Distributional Reinforcement Learning, **John D. Martin**,  Michal Lyskawinski, Xiaohu Li, Brendan Englot.   * [Biological and Artifical RL Workshop](https://sites.google.com/view/biologicalandartificialrl): Memento: Further Progress through Forgetting, William Fedus, Dibya Gosh, **John D. Martin**, Marc G. Bellemare, Hugo Larochelle, Yoshua Bengio.
* **August 30, 2019:** I'm giving a talk at [Google Robotics](https://ai.google/research/teams/brain/robotics/) in New York City on exploiting transition invariance for efficient Reinforcement Learning in multi-stage settings.
* **March 14, 2019:** I'm excited to announce that I will be working at [Google Brain](https://ai.google/research/teams/brain) in Montréal, Canada, as a research intern with Marc G. Bellemare's group. There I will study methods for continual reinforcement learning in deep neural networks, focusing on settings with non-stationary data. 

# Publications & Posters

* Stochastically Dominant Distributional Reinforcement Learning,  
**John D. Martin**,  Michal Lyskawinski, Xiaohu Li, Brendan Englot,  
*ArXiv* (2019)[[pdf]](https://arxiv.org/abs/1905.07318)

* Recursive Sparse Pseudo-input Gaussian Process SARSA,  
**John D. Martin**, Brendan Englot,  
*ArXiv* (2018)[[pdf]](https://arxiv.org/abs/1811.07201)

* Sparse Gaussian Process Temporal Difference Learning for Marine Robot Navigation,  
**John D. Martin**, Jinkun Wang, Brendan Englot,  
*Conference on Robot Learning* (2018) [[pdf](http://proceedings.mlr.press/v87/martin18a/martin18a.pdf)]

* Extending Model-based Policy Gradients for Robots in Heteroscedastic Environments,  
**John D. Martin**, Brendan Englot,  
*Conference on Robot Learning* (2017) [[pdf](http://proceedings.mlr.press/v78/martin17a/martin17a.pdf)]

* Distributed Gaussian Process Temporal Differences for Actor-critic Learning,  
**John D. Martin**, Zheng Xing, Zhiyuan Yao, Ionut Florescu, Brendan Englot,  
[*New York Academy of Sciences Machine Learning Symposium*](https://www.nyas.org/events/2018/12th-annual-machine-learning-symposium/?tab=description) (2018) [[poster](/publications/poster/2018-martin_xing_florescu_englot-nyas_mls_poster.pdf)]

* Predicting Ocean Currents for Robot Navigation,  
**John D. Martin**, Tixiao Shan, Brendan Englot,  
*Stevens Graduate Research Conference* (2017) [[poster](/publications/poster/2017-martin_shan_englot-predicting_ocean_currents_for_robot_navigation.pdf)]
